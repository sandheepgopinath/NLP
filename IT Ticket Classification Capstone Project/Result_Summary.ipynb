{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Result Summary.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOCJ8POAoCXz/pZ3RoHbckN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Capstone-Project-July/Interim/blob/master/Result_Summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCI_OSu_2L4M",
        "colab_type": "text"
      },
      "source": [
        "# **Summary of Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCrN4pg-2kEG",
        "colab_type": "text"
      },
      "source": [
        "> 1. After visualizing and analysing the given dataset, we have dropped the column \"Caller\" from the dataset and it is not used for further processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4yqW9EU2zIK",
        "colab_type": "text"
      },
      "source": [
        "> 2. On Analysing the target variable, we found that they were a large number of groups which didnt have enough data to train the model. \n",
        "\n",
        "  ie. There were many Assignment Groups which had very few tickets assigned to them. It was causing low accuracy for the models and these could be considered as outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyDZoZkl290p",
        "colab_type": "text"
      },
      "source": [
        "> 3. Inorder to fine tune the model, we have made two datasets. \n",
        "\n",
        "    > In the first dataset, we have dropped all the assignment groups which were having less than 50 tickets. \n",
        "\n",
        "    > In the second dataset,we have dropped all the assignment groups which were having less than 114 tickets.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr6EDHGc3mN5",
        "colab_type": "text"
      },
      "source": [
        "> 4. During the pre-processing, inorder to convert the content of the ticket into a vector we have tried two methods. \n",
        "\n",
        "    >In the first apporach, we have used TFIDF Vectorizer to convert the content into TFIDF vectors. \n",
        "    >In second approach, we have manually done the process by using tokenization, cleaning, TF vectors and TFIDF vectors. \n",
        "    > We found that both these approaches were giving similar cluster when we use KMeans.\n",
        "    >Hence, we have used TFIDF Vectorizer for further processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YNfzxv24DNd",
        "colab_type": "text"
      },
      "source": [
        "> 5. We have used KMeans Clustering to classify the entire dataset into three sub-clusters.\n",
        "\n",
        "    >  This helps in greatly increasing the accuracy of the classifying a ticket into any of the three pre-specified groups.\n",
        "    >  This also makes the model more usable in a real world situation as explained in the project report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VQCkFd34vci",
        "colab_type": "text"
      },
      "source": [
        "> 6. We have used an SGD Classifier, to classify an incoming ticket into any of these 3 clusters, with an accuracy of 98.5%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XXOm4v87-vp",
        "colab_type": "text"
      },
      "source": [
        "We have used two Notebooks for Model evaluation\n",
        "> Notebook 2.0 has a smaller dataset, where we have dropped more rows to make the dataset balanced\n",
        "\n",
        "> Notebook 2.1 is used to benchmark the results and has a bigger dataset with more datapoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgeKns9K5OlE",
        "colab_type": "text"
      },
      "source": [
        "    > For the inital classification, we have got an accuracy of 98.5% using an SGD Classifier\n",
        "\n",
        "    > For Cluster 1, we have got an accuracy of 96% using a Hypertuned SVM and it performs better than the benchmark accuracy of 94.4%\n",
        "\n",
        "    > For Cluster 2, we have got an accuracy of 70% using 1D Convolution, and it performs better than the benchmark accuracy of 61%\n",
        "\n",
        "    > For Cluster 3, we have got an accuracy of 77% using 1D SVM Classifier, and it performs better than the benchmark accuracy of 70.7%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXBXHNsq7MPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}